{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1933799/1865078928.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sae_weights = torch.load(sae_path)\n",
      "/tmp/ipykernel_1933799/1865078928.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(ckpt_path, map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DiffusionTransformerLowdimPolicy(\n",
       "  (model): TransformerForDiffusion(\n",
       "    (input_emb): Linear(in_features=2, out_features=256, bias=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (time_emb): SinusoidalPosEmb()\n",
       "    (cond_obs_emb): Linear(in_features=20, out_features=256, bias=True)\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "      (1): Mish()\n",
       "      (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.3, inplace=False)\n",
       "          (dropout2): Dropout(p=0.3, inplace=False)\n",
       "          (dropout3): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       "  (mask_generator): LowdimMaskGenerator()\n",
       "  (normalizer): LinearNormalizer(\n",
       "    (params_dict): ParameterDict(\n",
       "        (obs): Object of type: ParameterDict\n",
       "        (action): Object of type: ParameterDict\n",
       "      (obs): ParameterDict(\n",
       "          (offset): Parameter containing: [torch.cuda.FloatTensor of size 20 (cuda:0)]\n",
       "          (scale): Parameter containing: [torch.cuda.FloatTensor of size 20 (cuda:0)]\n",
       "          (input_stats): Object of type: ParameterDict\n",
       "        (input_stats): ParameterDict(\n",
       "            (max): Parameter containing: [torch.cuda.FloatTensor of size 20 (cuda:0)]\n",
       "            (mean): Parameter containing: [torch.cuda.FloatTensor of size 20 (cuda:0)]\n",
       "            (min): Parameter containing: [torch.cuda.FloatTensor of size 20 (cuda:0)]\n",
       "            (std): Parameter containing: [torch.cuda.FloatTensor of size 20 (cuda:0)]\n",
       "        )\n",
       "      )\n",
       "      (action): ParameterDict(\n",
       "          (offset): Parameter containing: [torch.cuda.FloatTensor of size 2 (cuda:0)]\n",
       "          (scale): Parameter containing: [torch.cuda.FloatTensor of size 2 (cuda:0)]\n",
       "          (input_stats): Object of type: ParameterDict\n",
       "        (input_stats): ParameterDict(\n",
       "            (max): Parameter containing: [torch.cuda.FloatTensor of size 2 (cuda:0)]\n",
       "            (mean): Parameter containing: [torch.cuda.FloatTensor of size 2 (cuda:0)]\n",
       "            (min): Parameter containing: [torch.cuda.FloatTensor of size 2 (cuda:0)]\n",
       "            (std): Parameter containing: [torch.cuda.FloatTensor of size 2 (cuda:0)]\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% model and sae loading\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "from sae.sae_model import SparseAutoencoder\n",
    "import torch \n",
    "import pandas as pd \n",
    "from tqdm.auto import tqdm\n",
    "from skvideo.io import vwrite\n",
    "from IPython.display import Video\n",
    "from diffusion_policy.policy.diffusion_transformer_lowdim_policy import DiffusionTransformerLowdimPolicy\n",
    "from diffusion_policy.model.diffusion.transformer_for_diffusion import TransformerForDiffusion\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import random\n",
    "from diffusion_policy.env.pusht.pusht_keypoints_env import PushTKeypointsEnv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "seed1_acts = pd.read_csv(\"data/activations_summary.csv\")\n",
    "seed2_acts = pd.read_csv(\"data/activations_summary_2.csv\")\n",
    "\n",
    "sae_path = \"sae/results_layer4_dim2048_k64_auxk64_dead200/checkpoints/last.ckpt\"\n",
    "sae_weights = torch.load(sae_path)\n",
    "ckpt = {}\n",
    "for k in sae_weights['state_dict'].keys():\n",
    "    if k.startswith('sae_model.'):\n",
    "        ckpt[k.split(\".\")[1]] = sae_weights['state_dict'][k]\n",
    "sae = SparseAutoencoder(256, 2048, 64, 64, 32, 200)\n",
    "sae.load_state_dict(ckpt)\n",
    "sae.to(device)\n",
    "\n",
    "ckpt_path = \"/work/pi_hzhang2_umass_edu/jnainani_umass_edu/Interp4Robotics/diffusionInterp/data/experiments/low_dim/pusht/diffusion_policy_transformer/train_2/checkpoints/latest.ckpt\"\n",
    "state_dict = torch.load(ckpt_path, map_location=device)\n",
    "config = state_dict['cfg']\n",
    "model_config = config['policy']['model']\n",
    "model_config = {k: v for k, v in model_config.items() if not k.startswith('_target_')}\n",
    "model = TransformerForDiffusion(**model_config)\n",
    "noise_scheduler_config = config['policy']['noise_scheduler']\n",
    "noise_scheduler = DDPMScheduler(**noise_scheduler_config)\n",
    "policy_params = {\n",
    "    'model': model,\n",
    "    'noise_scheduler': noise_scheduler,\n",
    "    'horizon': config['policy']['horizon'],\n",
    "    'obs_dim': config['policy']['obs_dim'],\n",
    "    'action_dim': config['policy']['action_dim'],\n",
    "    'n_action_steps': config['policy']['n_action_steps'],\n",
    "    'n_obs_steps': config['policy']['n_obs_steps'],\n",
    "    'num_inference_steps': config['policy'].get('num_inference_steps', None),\n",
    "    'obs_as_cond': config['policy'].get('obs_as_cond', False),\n",
    "    'pred_action_steps_only': config['policy'].get('pred_action_steps_only', False),\n",
    "}\n",
    "policy = DiffusionTransformerLowdimPolicy(**policy_params)\n",
    "policy.load_state_dict(state_dict['state_dicts']['model'])\n",
    "policy.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% helper functions\n",
    "\n",
    "\n",
    "def theta_tc(obs):\n",
    "    \"\"\"\n",
    "    Calculates the angle between the target block and the current block\n",
    "    \"\"\"\n",
    "    points_to_average = [\n",
    "    obs[8], obs[1] #, obs[2]\n",
    "    ]\n",
    "\n",
    "    # Convert to a NumPy array for easy computation\n",
    "    points_array = np.array(points_to_average)\n",
    "\n",
    "    # Compute the average\n",
    "    mid_pt = np.mean(points_array, axis=0)\n",
    "    # mid_pt = obs[3]\n",
    "    low_t = obs[4]\n",
    "    curr_block_theta = np.arctan2(-(mid_pt[1] - low_t[1]), mid_pt[0] - low_t[0])\n",
    "    theta_tc = curr_block_theta - np.pi / 4\n",
    "    return theta_tc, theta_tc * 180 / np.pi, curr_block_theta, curr_block_theta * 180 / np.pi\n",
    "\n",
    "def dist_tc(obs):\n",
    "    \"\"\"\n",
    "    Calculates the distance between the target block and the current block\n",
    "    \"\"\"\n",
    "    mid_pt_current = obs[3]\n",
    "    mid_pt_target = [48, 48]\n",
    "    dist = np.sqrt((mid_pt_current[0] - mid_pt_target[0])**2 + (mid_pt_current[1] - mid_pt_target[1])**2)\n",
    "    return dist\n",
    "\n",
    "def Ka(obs):\n",
    "    \"\"\"\n",
    "    Calculates the closest keypoint to agent \"\"\"\n",
    "    agent = obs[-1]\n",
    "    min_dist = np.inf\n",
    "    min_idx = -1\n",
    "    for idx, (x, y) in enumerate(obs):\n",
    "        if idx == 9:\n",
    "            continue\n",
    "        else:\n",
    "            dist = np.sqrt((x - agent[0])**2 + (y - agent[1])**2)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                min_idx = idx\n",
    "    return min_idx\n",
    "\n",
    "def theta_action(action):\n",
    "    \"\"\"\n",
    "    Calculates the change in angle of the first and last action\n",
    "    \"\"\"\n",
    "    first_angle = np.arctan2(- action[0][1] + action[1][1], action[0][0] - action[1][0])\n",
    "    last_angle = np.arctan2(- action[-2][1] + action[-1][1], action[-2][0] - action[-1][0])\n",
    "    change = last_angle - first_angle\n",
    "    return first_angle, last_angle, change, change * 180 / np.pi\n",
    "\n",
    "def dist_action(action):\n",
    "    \"\"\"\n",
    "    Calculates the distance between the first and last action\n",
    "    \"\"\"\n",
    "    dist = np.sqrt((action[0][0] - action[-1][0])**2 + (action[0][1] - action[-1][1])**2)\n",
    "    return dist\n",
    "\n",
    "def dist_action_mid(action, obs):\n",
    "    \"\"\"\n",
    "    Calculates the average distance between actions and midpoint of target block\n",
    "    \"\"\"\n",
    "    mid_pt_target = obs[3]\n",
    "    dist = 0\n",
    "    for idx, (x, y) in enumerate(action):\n",
    "        dist += np.sqrt((x - mid_pt_target[0])**2 + (y - mid_pt_target[1])**2)\n",
    "    return dist / len(action)\n",
    "\n",
    "def dist_action_target(action):\n",
    "    \"\"\"\n",
    "    Calculates the average distance between actions and midpoint of target block\n",
    "    \"\"\"\n",
    "    dist = 0\n",
    "    for idx, (x, y) in enumerate(action):\n",
    "        dist += np.sqrt((x - 48)**2 + (y - 48)**2)\n",
    "    return dist / len(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>step_idx</th>\n",
       "      <th>timestep</th>\n",
       "      <th>feature_36</th>\n",
       "      <th>feature_37</th>\n",
       "      <th>feature_43</th>\n",
       "      <th>feature_48</th>\n",
       "      <th>feature_60</th>\n",
       "      <th>feature_74</th>\n",
       "      <th>feature_81</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_123</th>\n",
       "      <th>feature_998</th>\n",
       "      <th>feature_353</th>\n",
       "      <th>feature_813</th>\n",
       "      <th>feature_1536</th>\n",
       "      <th>feature_708</th>\n",
       "      <th>feature_2005</th>\n",
       "      <th>feature_1007</th>\n",
       "      <th>feature_815</th>\n",
       "      <th>feature_1687</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17096</th>\n",
       "      <td>27242</td>\n",
       "      <td>112</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.307702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.074287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22898</th>\n",
       "      <td>30449</td>\n",
       "      <td>160</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.083910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22899</th>\n",
       "      <td>30449</td>\n",
       "      <td>160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17093</th>\n",
       "      <td>27242</td>\n",
       "      <td>112</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.125755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.120889</td>\n",
       "      <td>0.173829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22894</th>\n",
       "      <td>30449</td>\n",
       "      <td>160</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.117652</td>\n",
       "      <td>0.08486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.077789</td>\n",
       "      <td>0.081088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 930 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        seed  step_idx  timestep  feature_36  feature_37  feature_43  \\\n",
       "17096  27242       112       3.0    0.307702         NaN         NaN   \n",
       "22898  30449       160       1.0    0.083910         NaN         NaN   \n",
       "22899  30449       160       0.0         NaN         NaN         NaN   \n",
       "17093  27242       112       6.0    0.125755         NaN         NaN   \n",
       "22894  30449       160       5.0    0.117652     0.08486         NaN   \n",
       "\n",
       "       feature_48  feature_60  feature_74  feature_81  ...  feature_123  \\\n",
       "17096         NaN    0.074287         NaN         NaN  ...          NaN   \n",
       "22898         NaN         NaN         NaN         NaN  ...          NaN   \n",
       "22899         NaN         NaN         NaN         NaN  ...          NaN   \n",
       "17093    0.120889    0.173829         NaN         NaN  ...          NaN   \n",
       "22894    0.077789    0.081088         NaN         NaN  ...          NaN   \n",
       "\n",
       "       feature_998  feature_353  feature_813  feature_1536  feature_708  \\\n",
       "17096          NaN          NaN          NaN           NaN          NaN   \n",
       "22898          NaN          NaN          NaN           NaN          NaN   \n",
       "22899          NaN          NaN          NaN           NaN          NaN   \n",
       "17093          NaN          NaN          NaN           NaN          NaN   \n",
       "22894          NaN          NaN          NaN           NaN          NaN   \n",
       "\n",
       "       feature_2005  feature_1007  feature_815  feature_1687  \n",
       "17096           NaN           NaN          NaN           NaN  \n",
       "22898           NaN           NaN          NaN           NaN  \n",
       "22899           NaN           NaN          NaN           NaN  \n",
       "17093           NaN           NaN          NaN           NaN  \n",
       "22894           NaN           NaN          NaN           NaN  \n",
       "\n",
       "[5 rows x 930 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "better_acts = pd.read_csv(\"data/better_activations_summary.csv\")\n",
    "feature_idx = 922 \n",
    "better_acts.sort_values(\n",
    "    f\"feature_{feature_idx}\", ascending=False\n",
    ").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102400"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2048*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95300"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(better_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f922_30 = better_acts.sort_values(\n",
    "    f\"feature_{feature_idx}\", ascending=False\n",
    ")[['seed','step_idx','timestep', f'feature_{feature_idx}']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1933799/1431442656.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  inf_inputs = torch.load(f\"data/inference_inputs_seed_{int(seed)}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37.210842 62.80127 ]\n",
      " [39.729767 41.35932 ]\n",
      " [56.114487 57.294136]\n",
      " [44.37665  52.896835]\n",
      " [29.787546 64.14371 ]\n",
      " [48.689106 59.0212  ]\n",
      " [38.1087   48.662895]\n",
      " [37.26817  55.73711 ]\n",
      " [51.139698 52.359108]\n",
      " [52.965385 63.97052 ]]\n",
      "(0.04967745343909957, 2.846308419018062, 0.8350756, 47.846308419018065)\n",
      "6.091604768953897\n"
     ]
    }
   ],
   "source": [
    "seed, step, timestep, _ = f922_30[['seed', 'step_idx', 'timestep', f'feature_{feature_idx}']].values[0]\n",
    "# %%\n",
    "inf_inputs = torch.load(f\"data/inference_inputs_seed_{int(seed)}.pt\")\n",
    "# %%\n",
    "obs = torch.Tensor(inf_inputs[int(step)][0]['cond_input'])\n",
    "obs_norm1 = policy.normalizer['obs'].unnormalize(obs)[0][0]\n",
    "obs_norm2 = policy.normalizer['obs'].unnormalize(obs)[0][1]\n",
    "# %%\n",
    "points1 = np.array(obs_norm1.detach().cpu()).reshape(-1, 2)  / 512 * 96\n",
    "points2 = np.array(obs_norm2.detach().cpu()).reshape(-1, 2)  / 512 * 96\n",
    "print(points1)\n",
    "# %%\n",
    "print(theta_tc(points1))\n",
    "# %%\n",
    "print(dist_tc(points1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['timestep', 'trajectory_input', 'cond_input', 'trajectory_output'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_inputs[int(step)][int(timestep)].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 2]) torch.Size([1, 10, 2])\n"
     ]
    }
   ],
   "source": [
    "inp_action = torch.Tensor(inf_inputs[int(step)][int(timestep)]['trajectory_input'])\n",
    "out_action = torch.Tensor(inf_inputs[int(step)][int(timestep)]['trajectory_output'])\n",
    "print(inp_action.shape, out_action.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1886101/3652908117.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  inf_inputs = torch.load(f\"data/inference_inputs_seed_{int(seed)}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 27242.0, Step: 112.0, Timestep: 3.0\n",
      "Theta_tc: -5.924453607411201, d_tc: 5.892085507583355, ClosestK: 5 \n",
      " Delta_theta_act_in: 228.07397729338925 Delta_dist_act_out: 8.524491558907302 \n",
      " Delta_dist_act_in: 4.612924313909092 Delta_dist_act_out: 8.524491558907302 \n",
      " d_act_curr_in: 12.104566669028511 d_act_curr_out: 12.180254695448014 \n",
      " d_act_target_in: 15.469839999955132 d_act_target_out: 15.67071618704117\n",
      "\n",
      "\n",
      "Seed: 30449.0, Step: 160.0, Timestep: 1.0\n",
      "Theta_tc: -13.120717011274541, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: 2.857662356274125 Delta_dist_act_out: 7.796555249261298 \n",
      " Delta_dist_act_in: 8.82078920555269 Delta_dist_act_out: 7.796555249261298 \n",
      " d_act_curr_in: 13.831234245375066 d_act_curr_out: 13.755010382304548 \n",
      " d_act_target_in: 16.247847524461836 d_act_target_out: 15.868393522531587\n",
      "\n",
      "\n",
      "Seed: 30449.0, Step: 160.0, Timestep: 0.0\n",
      "Theta_tc: -13.120717011274541, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: -73.89350802642849 Delta_dist_act_out: 6.8138331835571355 \n",
      " Delta_dist_act_in: 7.796555249261298 Delta_dist_act_out: 6.8138331835571355 \n",
      " d_act_curr_in: 13.755010382304548 d_act_curr_out: 13.503240927422599 \n",
      " d_act_target_in: 15.868393522531587 d_act_target_out: 15.520998375859582\n",
      "\n",
      "\n",
      "Seed: 27242.0, Step: 112.0, Timestep: 6.0\n",
      "Theta_tc: -5.924453607411201, d_tc: 5.892085507583355, ClosestK: 5 \n",
      " Delta_theta_act_in: -37.0210391506273 Delta_dist_act_out: 8.009703604550946 \n",
      " Delta_dist_act_in: 3.7091172114748474 Delta_dist_act_out: 8.009703604550946 \n",
      " d_act_curr_in: 10.340604738909029 d_act_curr_out: 11.074008723963415 \n",
      " d_act_target_in: 14.384016292012882 d_act_target_out: 15.056624229773144\n",
      "\n",
      "\n",
      "Seed: 30449.0, Step: 160.0, Timestep: 5.0\n",
      "Theta_tc: -13.120717011274541, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: 20.106273746576655 Delta_dist_act_out: 12.08502802678232 \n",
      " Delta_dist_act_in: 6.484109555375557 Delta_dist_act_out: 12.08502802678232 \n",
      " d_act_curr_in: 14.539152801693138 d_act_curr_out: 13.843240651442585 \n",
      " d_act_target_in: 16.887147508851697 d_act_target_out: 16.43755218360552\n",
      "\n",
      "\n",
      "Seed: 29142.0, Step: 64.0, Timestep: 0.0\n",
      "Theta_tc: -6.890969771197611, d_tc: 6.771475108281929, ClosestK: 5 \n",
      " Delta_theta_act_in: 189.6490678909467 Delta_dist_act_out: 8.2699769182921 \n",
      " Delta_dist_act_in: 7.231006687803043 Delta_dist_act_out: 8.2699769182921 \n",
      " d_act_curr_in: 8.502619600503728 d_act_curr_out: 8.70440743091989 \n",
      " d_act_target_in: 14.685435257476257 d_act_target_out: 14.946781738083445\n",
      "\n",
      "\n",
      "Seed: 30449.0, Step: 160.0, Timestep: 4.0\n",
      "Theta_tc: -13.120717011274541, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: -130.5625084001354 Delta_dist_act_out: 11.63731273551029 \n",
      " Delta_dist_act_in: 12.08502802678232 Delta_dist_act_out: 11.63731273551029 \n",
      " d_act_curr_in: 13.843240651442585 d_act_curr_out: 13.558133692268143 \n",
      " d_act_target_in: 16.43755218360552 d_act_target_out: 16.137495477422338\n",
      "\n",
      "\n",
      "Seed: 30449.0, Step: 160.0, Timestep: 3.0\n",
      "Theta_tc: -13.120717011274541, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: 244.77949985221656 Delta_dist_act_out: 10.65563884275428 \n",
      " Delta_dist_act_in: 11.63731273551029 Delta_dist_act_out: 10.65563884275428 \n",
      " d_act_curr_in: 13.558133692268143 d_act_curr_out: 13.509027672486244 \n",
      " d_act_target_in: 16.137495477422338 d_act_target_out: 16.13532654724354\n",
      "\n",
      "\n",
      "Seed: 27242.0, Step: 112.0, Timestep: 1.0\n",
      "Theta_tc: -5.924453607411201, d_tc: 5.892085507583355, ClosestK: 5 \n",
      " Delta_theta_act_in: -37.695264199072184 Delta_dist_act_out: 9.887271064046287 \n",
      " Delta_dist_act_in: 10.93645687419492 Delta_dist_act_out: 9.887271064046287 \n",
      " d_act_curr_in: 11.789672254451443 d_act_curr_out: 11.25740821387515 \n",
      " d_act_target_in: 15.145762115949825 d_act_target_out: 14.954229363979746\n",
      "\n",
      "\n",
      "Seed: 27242.0, Step: 112.0, Timestep: 0.0\n",
      "Theta_tc: -5.924453607411201, d_tc: 5.892085507583355, ClosestK: 5 \n",
      " Delta_theta_act_in: 0.7149500513710847 Delta_dist_act_out: 8.875575521561096 \n",
      " Delta_dist_act_in: 9.887271064046287 Delta_dist_act_out: 8.875575521561096 \n",
      " d_act_curr_in: 11.25740821387515 d_act_curr_out: 11.417218629752242 \n",
      " d_act_target_in: 14.954229363979746 d_act_target_out: 15.20157547913433\n",
      "\n",
      "\n",
      "Seed: 27242.0, Step: 112.0, Timestep: 4.0\n",
      "Theta_tc: -5.924453607411201, d_tc: 5.892085507583355, ClosestK: 5 \n",
      " Delta_theta_act_in: -35.60927320013152 Delta_dist_act_out: 4.612924313909092 \n",
      " Delta_dist_act_in: 0.6932237691476434 Delta_dist_act_out: 4.612924313909092 \n",
      " d_act_curr_in: 11.10866556953103 d_act_curr_out: 12.104566669028511 \n",
      " d_act_target_in: 14.640125498151459 d_act_target_out: 15.469839999955132\n",
      "\n",
      "\n",
      "Seed: 27242.0, Step: 112.0, Timestep: 5.0\n",
      "Theta_tc: -5.924453607411201, d_tc: 5.892085507583355, ClosestK: 5 \n",
      " Delta_theta_act_in: 22.27730401577857 Delta_dist_act_out: 0.6932237691476434 \n",
      " Delta_dist_act_in: 8.009703604550946 Delta_dist_act_out: 0.6932237691476434 \n",
      " d_act_curr_in: 11.074008723963415 d_act_curr_out: 11.10866556953103 \n",
      " d_act_target_in: 15.056624229773144 d_act_target_out: 14.640125498151459\n",
      "\n",
      "\n",
      "Seed: 27242.0, Step: 112.0, Timestep: 8.0\n",
      "Theta_tc: -5.924453607411201, d_tc: 5.892085507583355, ClosestK: 5 \n",
      " Delta_theta_act_in: 86.24100106457249 Delta_dist_act_out: 0.6058496699434579 \n",
      " Delta_dist_act_in: 5.179235655081106 Delta_dist_act_out: 0.6058496699434579 \n",
      " d_act_curr_in: 13.359131904129077 d_act_curr_out: 12.594198056430205 \n",
      " d_act_target_in: 17.20115398656015 d_act_target_out: 16.330676678347125\n",
      "\n",
      "\n",
      "Seed: 27242.0, Step: 112.0, Timestep: 7.0\n",
      "Theta_tc: -5.924453607411201, d_tc: 5.892085507583355, ClosestK: 5 \n",
      " Delta_theta_act_in: -47.19475618352303 Delta_dist_act_out: 3.7091172114748474 \n",
      " Delta_dist_act_in: 0.6058496699434579 Delta_dist_act_out: 3.7091172114748474 \n",
      " d_act_curr_in: 12.594198056430205 d_act_curr_out: 10.340604738909029 \n",
      " d_act_target_in: 16.330676678347125 d_act_target_out: 14.384016292012882\n",
      "\n",
      "\n",
      "Seed: 30449.0, Step: 160.0, Timestep: 7.0\n",
      "Theta_tc: -13.120717011274541, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: -3.7344605702178315 Delta_dist_act_out: 11.070572051472405 \n",
      " Delta_dist_act_in: 13.385885909981637 Delta_dist_act_out: 11.070572051472405 \n",
      " d_act_curr_in: 16.059992565256668 d_act_curr_out: 13.98007051090357 \n",
      " d_act_target_in: 17.765852195981374 d_act_target_out: 16.136958418889932\n",
      "\n",
      "\n",
      "Seed: 30449.0, Step: 160.0, Timestep: 9.0\n",
      "Theta_tc: -13.120717011274541, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: -21.697598540162907 Delta_dist_act_out: 9.96262957935242 \n",
      " Delta_dist_act_in: 14.606620336671067 Delta_dist_act_out: 9.96262957935242 \n",
      " d_act_curr_in: 15.371704823656506 d_act_curr_out: 16.26224679633916 \n",
      " d_act_target_in: 17.023595875520027 d_act_target_out: 17.6078025569621\n",
      "\n",
      "\n",
      "Seed: 30449.0, Step: 160.0, Timestep: 11.0\n",
      "Theta_tc: -13.120717011274541, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: -312.40656886234683 Delta_dist_act_out: 19.242723078667055 \n",
      " Delta_dist_act_in: 24.348388228334645 Delta_dist_act_out: 19.242723078667055 \n",
      " d_act_curr_in: 14.504220183618012 d_act_curr_out: 14.87243887344297 \n",
      " d_act_target_in: 16.039721469511413 d_act_target_out: 16.698790998209123\n",
      "\n",
      "\n",
      "Seed: 30449.0, Step: 160.0, Timestep: 6.0\n",
      "Theta_tc: -13.120717011274541, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: -52.57557238146991 Delta_dist_act_out: 6.484109555375557 \n",
      " Delta_dist_act_in: 11.070572051472405 Delta_dist_act_out: 6.484109555375557 \n",
      " d_act_curr_in: 13.98007051090357 d_act_curr_out: 14.539152801693138 \n",
      " d_act_target_in: 16.136958418889932 d_act_target_out: 16.887147508851697\n",
      "\n",
      "\n",
      "Seed: 27242.0, Step: 112.0, Timestep: 2.0\n",
      "Theta_tc: -5.924453607411201, d_tc: 5.892085507583355, ClosestK: 5 \n",
      " Delta_theta_act_in: -35.47701341704339 Delta_dist_act_out: 10.93645687419492 \n",
      " Delta_dist_act_in: 8.524491558907302 Delta_dist_act_out: 10.93645687419492 \n",
      " d_act_curr_in: 12.180254695448014 d_act_curr_out: 11.789672254451443 \n",
      " d_act_target_in: 15.67071618704117 d_act_target_out: 15.145762115949825\n",
      "\n",
      "\n",
      "Seed: 54206.0, Step: 176.0, Timestep: 0.0\n",
      "Theta_tc: -5.437099119563282, d_tc: 4.934497913294157, ClosestK: 5 \n",
      " Delta_theta_act_in: 124.91163335259542 Delta_dist_act_out: 2.104227806232519 \n",
      " Delta_dist_act_in: 2.2139130457955862 Delta_dist_act_out: 2.104227806232519 \n",
      " d_act_curr_in: 7.563339340032795 d_act_curr_out: 7.492877399128761 \n",
      " d_act_target_in: 11.688199968786515 d_act_target_out: 11.716550574858699\n",
      "\n",
      "\n",
      "Seed: 87390.0, Step: 120.0, Timestep: 6.0\n",
      "Theta_tc: -7.0373885364351745, d_tc: 5.5391783021719645, ClosestK: 2 \n",
      " Delta_theta_act_in: -87.87767148405844 Delta_dist_act_out: 29.703067223087928 \n",
      " Delta_dist_act_in: 28.742530859944598 Delta_dist_act_out: 29.703067223087928 \n",
      " d_act_curr_in: 18.910381359509774 d_act_curr_out: 18.296849076738074 \n",
      " d_act_target_in: 20.741730713198688 d_act_target_out: 20.429597026139042\n",
      "\n",
      "\n",
      "Seed: 30449.0, Step: 160.0, Timestep: 2.0\n",
      "Theta_tc: -13.120717011274541, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: 179.81620461839128 Delta_dist_act_out: 8.82078920555269 \n",
      " Delta_dist_act_in: 10.65563884275428 Delta_dist_act_out: 8.82078920555269 \n",
      " d_act_curr_in: 13.509027672486244 d_act_curr_out: 13.831234245375066 \n",
      " d_act_target_in: 16.13532654724354 d_act_target_out: 16.247847524461836\n",
      "\n",
      "\n",
      "Seed: 30449.0, Step: 160.0, Timestep: 13.0\n",
      "Theta_tc: -13.120717011274541, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: 208.55876079682884 Delta_dist_act_out: 30.886322338117857 \n",
      " Delta_dist_act_in: 29.975066981754274 Delta_dist_act_out: 30.886322338117857 \n",
      " d_act_curr_in: 14.961675028680514 d_act_curr_out: 14.329276780624436 \n",
      " d_act_target_in: 16.69347257862457 d_act_target_out: 15.534042415246962\n",
      "\n",
      "\n",
      "Seed: 30449.0, Step: 160.0, Timestep: 18.0\n",
      "Theta_tc: -13.120717011274541, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: 43.51333544730494 Delta_dist_act_out: 15.681754916348817 \n",
      " Delta_dist_act_in: 14.349749125291243 Delta_dist_act_out: 15.681754916348817 \n",
      " d_act_curr_in: 18.568564163763817 d_act_curr_out: 19.816827041684576 \n",
      " d_act_target_in: 21.93210458024658 d_act_target_out: 22.875942943062775\n",
      "\n",
      "\n",
      "Seed: 87390.0, Step: 120.0, Timestep: 9.0\n",
      "Theta_tc: -7.0373885364351745, d_tc: 5.5391783021719645, ClosestK: 2 \n",
      " Delta_theta_act_in: -82.12694590104202 Delta_dist_act_out: 38.52140260352075 \n",
      " Delta_dist_act_in: 38.7100072057115 Delta_dist_act_out: 38.52140260352075 \n",
      " d_act_curr_in: 20.405612234719484 d_act_curr_out: 20.215646039002966 \n",
      " d_act_target_in: 22.440935284080393 d_act_target_out: 22.079848169890944\n",
      "\n",
      "\n",
      "Seed: 87390.0, Step: 32.0, Timestep: 1.0\n",
      "Theta_tc: -6.759949667443841, d_tc: 9.360211589705171, ClosestK: 5 \n",
      " Delta_theta_act_in: 78.82597011668821 Delta_dist_act_out: 7.056327403316638 \n",
      " Delta_dist_act_in: 6.171370983274671 Delta_dist_act_out: 7.056327403316638 \n",
      " d_act_curr_in: 11.360120361236238 d_act_curr_out: 11.69672865083152 \n",
      " d_act_target_in: 19.428052320533016 d_act_target_out: 19.756804991078326\n",
      "\n",
      "\n",
      "Seed: 87390.0, Step: 112.0, Timestep: 0.0\n",
      "Theta_tc: -11.981837608688275, d_tc: 5.933877398215284, ClosestK: 2 \n",
      " Delta_theta_act_in: 188.68680351991864 Delta_dist_act_out: 3.5961332652021367 \n",
      " Delta_dist_act_in: 2.4054262709840417 Delta_dist_act_out: 3.5961332652021367 \n",
      " d_act_curr_in: 12.58587357579929 d_act_curr_out: 13.015501996152642 \n",
      " d_act_target_in: 14.88765171285113 d_act_target_out: 15.416625467630457\n",
      "\n",
      "\n",
      "Seed: 27242.0, Step: 112.0, Timestep: 12.0\n",
      "Theta_tc: -5.924453607411201, d_tc: 5.892085507583355, ClosestK: 5 \n",
      " Delta_theta_act_in: 195.64892462393934 Delta_dist_act_out: 3.4353188687925775 \n",
      " Delta_dist_act_in: 9.82514268385188 Delta_dist_act_out: 3.4353188687925775 \n",
      " d_act_curr_in: 13.284365167456937 d_act_curr_out: 12.744340742384711 \n",
      " d_act_target_in: 16.612259172185798 d_act_target_out: 15.919195620453314\n",
      "\n",
      "\n",
      "Seed: 98355.0, Step: 48.0, Timestep: 0.0\n",
      "Theta_tc: -3.4485305238490893, d_tc: 8.548137406439595, ClosestK: 5 \n",
      " Delta_theta_act_in: 42.03390622779642 Delta_dist_act_out: 10.58220800848578 \n",
      " Delta_dist_act_in: 9.142255258227745 Delta_dist_act_out: 10.58220800848578 \n",
      " d_act_curr_in: 9.36780635406041 d_act_curr_out: 8.95502729205292 \n",
      " d_act_target_in: 17.1714397914295 d_act_target_out: 16.806456177498067\n",
      "\n",
      "\n",
      "Seed: 29142.0, Step: 64.0, Timestep: 5.0\n",
      "Theta_tc: -6.890969771197611, d_tc: 6.771475108281929, ClosestK: 5 \n",
      " Delta_theta_act_in: 227.60168337266123 Delta_dist_act_out: 14.779955292108616 \n",
      " Delta_dist_act_in: 14.645413321187199 Delta_dist_act_out: 14.779955292108616 \n",
      " d_act_curr_in: 8.107426389080683 d_act_curr_out: 6.559899425166416 \n",
      " d_act_target_in: 12.803348908221599 d_act_target_out: 11.34535480314307\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, row in f922_30.iterrows():\n",
    "    seed, step, timestep, _ = row.values\n",
    "    inf_inputs = torch.load(f\"data/inference_inputs_seed_{int(seed)}.pt\")\n",
    "    time_ind = 99 - int(timestep)\n",
    "    obs = torch.Tensor(inf_inputs[int(step)][time_ind]['cond_input'])\n",
    "    action_in = torch.Tensor(inf_inputs[int(step)][time_ind]['trajectory_input'])\n",
    "    action_out = torch.Tensor(inf_inputs[int(step)][time_ind]['trajectory_output'])\n",
    "    obs_norm1 = policy.normalizer['obs'].unnormalize(obs)[0][0]\n",
    "    obs_norm2 = policy.normalizer['obs'].unnormalize(obs)[0][1]\n",
    "    inp_action = np.array(policy.normalizer['action'].unnormalize(action_in)[0].detach().cpu()) / 512 * 96\n",
    "    out_action = np.array(policy.normalizer['action'].unnormalize(action_out)[0].detach().cpu()) / 512 * 96\n",
    "    points1 = np.array(obs_norm1.detach().cpu()).reshape(-1, 2)  / 512 * 96\n",
    "    points2 = np.array(obs_norm2.detach().cpu()).reshape(-1, 2)  / 512 * 96\n",
    "    print(f\"Seed: {seed}, Step: {step}, Timestep: {timestep}\")\n",
    "    # print(theta_tc(points1))\n",
    "    _, theta_tc_deg, _, theta_c_deg = theta_tc(points2)\n",
    "    # print(dist_tc(points1))\n",
    "    d_tc = dist_tc(points2)\n",
    "    ClosestK = Ka(points2)\n",
    "    # print(Ka(points2))\n",
    "    _, _, _, Delta_theta_act_in = theta_action(inp_action)\n",
    "    _, _, _, Delta_theta_act_out = theta_action(out_action)\n",
    "    Delta_dist_act_in = dist_action(inp_action)\n",
    "    Delta_dist_act_out = dist_action(out_action)\n",
    "    # print(dist_action_mid(inp_action, points1))\n",
    "    d_act_curr_in = dist_action_mid(inp_action, points2)\n",
    "    d_act_curr_out = dist_action_mid(out_action, points2)\n",
    "    d_act_target_in = dist_action_target(inp_action)\n",
    "    d_act_target_out = dist_action_target(out_action)\n",
    "    print(f\"Theta_tc: {theta_tc_deg}, d_tc: {d_tc}, ClosestK: {ClosestK} \\n Delta_theta_act_in: {Delta_theta_act_in} Delta_dist_act_out: {Delta_dist_act_out} \\n Delta_dist_act_in: {Delta_dist_act_in} Delta_dist_act_out: {Delta_dist_act_out} \\n d_act_curr_in: {d_act_curr_in} d_act_curr_out: {d_act_curr_out} \\n d_act_target_in: {d_act_target_in} d_act_target_out: {d_act_target_out}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_hzhang2_umass_edu/jnainani_umass_edu/diffusion_policy_interp/diffusion_policy/env/pusht/pymunk_keypoint_manager.py:37: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap('tab10')\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABgAGADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3mONWTc24kk/xH1p3kp/tf99GiH/V/if51JQBH5Kf7X/fRo8lP9r/AL6NSUUAR+Sn+1/30aPJT/a/76NSUyOWOZS0UiOoYqSrAgEHBH1BGKAE8lP9r/vo0eSn+1/30akooAj8lP8Aa/76NHkp/tf99GpKKAI/JT/a/wC+jTZI1VNy7gQR/EfWpqjm/wBX+I/nQAQ/6v8AE/zqSo4f9X+J/nUlABRRRQBXv7Y3unXVqspiaeJ4xIBkpuBGfwzXPeDfDN14eS7a7mhZ5ygCw5IAXPOSB/ePGO3fPHU0VtGvONOVJbPf5EOCclJ7oKKKKxLCiiigAqOb/V/iP51JUc3+r/EfzoAIf9X+J/nUlRw/6v8AE/zqSgAooooA8+8bIra+m5Q3+jIfm5x8zjj06Cur8Mz/AGjw5YttxsTyuufuErn/AMdrA8dqoutOcKAzJKCcckApgfqfzNS+ENWEdkbOfAjSVhG/pnDc/ixrSckqabMHNQl7x2FFNV1YsFYEqcMAehxnB/Aj86dWZuFFFFABUc3+r/EfzqSo5v8AV/iP50AEP+r/ABP86kqOH/V/if51JQAVz/i2/vdOsbaazuDCWm8tsIrZBVj3B9K6CoLy3t7i3Zbm2S4RfmEboHyR6A96aaTuxS2PLb7VbnUyi3l60xU/Ku4Lz9FwM1FHp8szlUspnYckmFjgepJHT3rtHe41a8CqvH8K/wAKCuhsrKOyh2Jyx+856sainipTfuR0OGnKVWT5du5yPg92066uluojEswjVGJBGRu9D7iu3rO/sa2+2+fj5OvlY4z/AIe3/wCqtGlF1G25nRRVRJqYUUUVZuFRzf6v8R/OpKjm/wBX+I/nQAQ/6v8AE/zqSo4f9X+J/nUlABRRRQBHHDHE7siKrSHLEDqakoooSsJJLYKKKKBhRRRQAVHN/q/xH86kqOb/AFf4j+dABD/q/wAT/OpKhjkVU2tuBBP8J9ad5yf7X/fJoAkoqPzk/wBr/vk0ecn+1/3yaAJKKj85P9r/AL5NHnJ/tf8AfJoAkoqPzk/2v++TR5yf7X/fJoAkoqPzk/2v++TR5yf7X/fJoAkqOb/V/iP50ecn+1/3yabJIrJtXcSSP4T60Af/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGAAAABgCAIAAABt+uBvAAAEvElEQVR4Ae2dTU8TQRjH2XZLw1tbIhSpCfoduHoyfgP0TmLCwRCJJ7+CJw0XDhgSPCrhQGJiosbExJeT3ryi8lKklJS2lEK6tP7rJtvZ2ekuJs5Lw2M4zPPMlH3m1/l1tkMRq9VqnZ2d9dE/EYFkMmmDTj6fHxoaEg241LlarZbL5WysINDJZrOXGoZo8oVCAXBioi7KdQjELMvqRNTyEwCcGFaRP0lRhwAp1mHRrUWKdSPTzpNiYXTQR4pFAEI3KRbGiBQLo4M+UiwCELpJsTBGpFgYHfTpVOyoen7r/tbPvUZEjbq7bawi9TUsvii9+twYGZ96sNQ8Ptx6vzSlvoaLXFGbYhsfqoOZ9gGLFYv1D6YvUquWMdoUi8UT3oSTQ2mTRdOzizXPOy891YOtG5MdXh44ExraFMOLDri4X8a+AOEZgmK2rifKZC4sEz2KsRWY3NammMlQ2Nq07WJsEYa3SbGwJ4gUC6ODPlIsAhC6SbEwRqRYGB30kWIRgNBNioUxgmLtT3eEDfl/fU9W1grFEvv9Hj+aY0MD24Cj6L3Yu49fGy376rXrHoXfu7+8tskNRYrdvjldOjzgQOz7FxTXa0KocxcbHjH3INF7bnTuYsOpzNOVNa8UYxuKFDN2/uGF6VQsvDJDenUqZgiCyDLUKTYxNnpSq0YWZNQApYo9vHf35LjHAKlWzHE6P+3BSrHtxMr2c6OWTLAYdYoFrz02kfvx+nS9uh7sMiSjVDHhnJ2StePsrJZXhb3ak6oV6zbhcrNsLCOdirG8wMhA1zQoFrLTs67hbITFp6sNxax6vV6pVNT8tg/evj97+QbvwtwJO45T3N/lJo+tDS/eSOI8RO+BEX7bJ5VKKfoA1ebW3uZ2HqdCmPlxtexCASn2hIgjZUIIxaScKH759n3j7Sd2hmPZnJ1IGI6DLdhtQzEpJ4q1k1NcoOdwBAEhI2UXw/mh8GL/mtS+r2nYxSIZFffzeHl2T6zZfS3ygTIGyFIsslan0X5TVizkvZF2PD6QTgzfKY17qb8N9x5yNj3rT6uLpO9iuPFpNpvHlSN2TiPTzvB0Y5JJLYwuIFo+Wq636ky63XTvIWdGZri8glDWLobSr2RSriY45cBJkDuZxdKicFYuHXTNZeaEY+Ca8IGyk+puFLEusDqC84n3xedH57k8t46EY7iHyAjdG0UpuxhXbjc6GBakgyTW0YA14H0T4RivV2pD0S4mXDuYmGdWcJJg5CZDxgQf9d8z0nexbmvnItboReOxlqhYNzq4tkZrvJlfpCFXsf6+fmERhiwNYW1cEopJ/L874lacYwGzuAxXkIGhRMXc2bJEesUs73mSq5h3mflM+06HJeV1Gd5Qd6NoOAhheepuFIWX74mkIsV6goWwSLm7mPCSPZeUvov1HBG2YFKMpSFok2ICKFyKFOOA+EJSzIcjGJBiQSZ8hhTjibAxKcbSELRJMQEULkWKcUB8ISnmwxEMSLEgEz5DivFE2JgUY2kI2qSYAAqXIsU4IL6QFPPhCAakWJAJnyHFeCJsTIqxNARtUkwAhUuRYhwQX0iK+XAEA1IsyITPtD8GjL/yg59D8z2XPgaWdDptYRXRn8/qthjw57P+ANN68lp74NRnAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=96x96>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "env = PushTKeypointsEnv()\n",
    "env.seed(0)\n",
    "env.reset()\n",
    "img = Image.fromarray(env.render(mode='rgb_array'))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from PIL import Image\n",
    "import collections\n",
    "import numpy as np\n",
    "import pygame\n",
    "import pymunk\n",
    "import pymunk.pygame_util\n",
    "from pymunk.vec2d import Vec2d\n",
    "import shapely.geometry as sg\n",
    "import cv2\n",
    "import os\n",
    "import skimage.transform as st\n",
    "from diffusion_policy.env.pusht.pymunk_override import DrawOptions\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def get_image_from_obs(points, action, seed, step, timestep, action_mode=\"Input\"):\n",
    "    center_x, center_y = 96 / 2, 96 / 2\n",
    "    d_i = 30\n",
    "    angle = (5 * np.pi) / 4\n",
    "    end_x = center_x + d_i * np.cos(angle)\n",
    "    end_y = center_y - d_i * np.sin(angle)\n",
    "    points_to_average = [\n",
    "    points[8], points[1] \n",
    "    ]\n",
    "    points_array = np.array(points_to_average)\n",
    "    average_point = np.mean(points_array, axis=0)\n",
    "\n",
    "    env = PushTKeypointsEnv()\n",
    "    canvas = pygame.Surface((env.window_size, env.window_size))\n",
    "    canvas.fill((255, 255, 255))\n",
    "    env.screen = canvas\n",
    "    draw_options = DrawOptions(canvas)\n",
    "    # Draw goal pose.\n",
    "    env.space = pymunk.Space()\n",
    "    goal_body = env._get_goal_pose_body(np.array([256,256,np.pi/4]))\n",
    "    env.goal_color = pygame.Color('LightGreen')\n",
    "    block_temp = env.add_tee((256, 300), 0)\n",
    "    for shape in block_temp.shapes:\n",
    "        goal_points = [pymunk.pygame_util.to_pygame(goal_body.local_to_world(v), draw_options.surface) for v in shape.get_vertices()]\n",
    "        goal_points += [goal_points[0]]\n",
    "        pygame.draw.polygon(canvas, env.goal_color, goal_points)\n",
    "    img = np.transpose(\n",
    "                    np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "                )\n",
    "    img = cv2.resize(img, (env.render_size, env.render_size))\n",
    "    img = Image.fromarray(img)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Overlay the points and annotate them\n",
    "    for idx, (x, y) in enumerate(points):\n",
    "        if idx == 9:\n",
    "            plt.scatter(x, y, c='blue', label='Agent start')\n",
    "        else:\n",
    "            plt.scatter(x, y, c='red', label=f'Point {idx}' if idx == 0 else None)  # Points now match image size\n",
    "            plt.text(x, y, f'{idx}', fontsize=9, color='blue')\n",
    "\n",
    "    # Plot arrows for the action trajectory with gradient colors\n",
    "    num_actions = len(action) - 1\n",
    "    colors = cm.viridis(np.linspace(0, 1, num_actions)) \n",
    "    for i in range(num_actions):\n",
    "        start_x, start_y = action[i]\n",
    "        end_x, end_y = action[i + 1]\n",
    "        dx = end_x - start_x\n",
    "        dy = end_y - start_y\n",
    "        if i == 0:\n",
    "            plt.quiver(start_x, start_y, dx, dy, angles='xy', scale_units='xy', scale=1, color=colors[i], label=action_mode+\" start\")\n",
    "        elif i == num_actions - 1:\n",
    "            plt.quiver(start_x, start_y, dx, dy, angles='xy', scale_units='xy', scale=1, color=colors[i], label=action_mode+\" end\")\n",
    "        else:\n",
    "            plt.quiver(start_x, start_y, dx, dy, angles='xy', scale_units='xy', scale=1, color=colors[i])\n",
    "\n",
    "    top_left = points[1]     # Left side of the base\n",
    "    top_right = points[2]    # Right side of the base\n",
    "    center_top = points[4]   # Top of the T (center vertical bar)\n",
    "    mid_point = average_point    # Middle connecting point\n",
    "    # Draw the base of the T (horizontal line)\n",
    "    plt.plot([top_left[0], top_right[0]], [top_left[1], top_right[1]], c='grey', linewidth=2)\n",
    "    # Draw the vertical bar of the T\n",
    "    plt.plot([mid_point[0], center_top[0]], [mid_point[1], center_top[1]], c='grey', linewidth=2)\n",
    "    # Plot the center of the image\n",
    "    plt.scatter(center_x, center_y, c='green', label='Center')  # \n",
    "    plt.legend(loc='upper left')\n",
    "    # plt.title(\"Points and Line Overlay on Image\")\n",
    "    plt.savefig(f\"sae_analysis/out/env_imgs/seed{seed}_step{step}_timestep{timestep}_action{action_mode}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Define a function to compute statistics\n",
    "def compute_stats(row, policy, save_img=True):\n",
    "    seed, step, timestep, activation = row.values\n",
    "    seed, step, timestep = int(seed), int(step), int(timestep)\n",
    "    inf_inputs = torch.load(f\"data/inference_inputs_seed_{int(seed)}.pt\", weights_only=True)\n",
    "    time_ind = 99 - int(timestep)\n",
    "    obs = torch.Tensor(inf_inputs[int(step)][time_ind]['cond_input'])\n",
    "    action_in = torch.Tensor(inf_inputs[int(step)][time_ind]['trajectory_input'])\n",
    "    action_out = torch.Tensor(inf_inputs[int(step)][time_ind]['trajectory_output'])\n",
    "    obs_norm1 = policy.normalizer['obs'].unnormalize(obs)[0][0]\n",
    "    obs_norm2 = policy.normalizer['obs'].unnormalize(obs)[0][1]\n",
    "    inp_action = np.array(policy.normalizer['action'].unnormalize(action_in)[0].detach().cpu()) / 512 * 96\n",
    "    out_action = np.array(policy.normalizer['action'].unnormalize(action_out)[0].detach().cpu()) / 512 * 96\n",
    "    points1 = np.array(obs_norm1.detach().cpu()).reshape(-1, 2) / 512 * 96\n",
    "    points2 = np.array(obs_norm2.detach().cpu()).reshape(-1, 2) / 512 * 96\n",
    "    _, theta_tc_deg, _, theta_c_deg = theta_tc(points2)\n",
    "    d_tc = dist_tc(points2)\n",
    "    ClosestK = Ka(points2)\n",
    "    _, _, _, Delta_theta_act_in = theta_action(inp_action)\n",
    "    _, _, _, Delta_theta_act_out = theta_action(out_action)\n",
    "    Delta_dist_act_in = dist_action(inp_action)\n",
    "    Delta_dist_act_out = dist_action(out_action)\n",
    "    # print(dist_action_mid(inp_action, points1))\n",
    "    d_act_curr_in = dist_action_mid(inp_action, points2)\n",
    "    d_act_curr_out = dist_action_mid(out_action, points2)\n",
    "    d_act_target_in = dist_action_target(inp_action)\n",
    "    d_act_target_out = dist_action_target(out_action)\n",
    "\n",
    "    img_saved = os.path.exists(f\"sae_analysis/out/env_imgs/seed{seed}_step{step}_timestep{timestep}_actionInput.png\")\n",
    "    if save_img and not img_saved:\n",
    "        get_image_from_obs(points2, inp_action, seed, step, timestep, \"Input\")\n",
    "        get_image_from_obs(points2, out_action, seed, step, timestep, \"Output\")\n",
    "\n",
    "    # Collect the results in a dictionary\n",
    "    return {\n",
    "        \"seed\": int(seed),\n",
    "        \"step_idx\": int(step),\n",
    "        \"timestep\": time_ind,\n",
    "        \"activation\": activation,\n",
    "        \"theta_tc_deg\": theta_tc_deg,\n",
    "        \"theta_c_deg\": theta_c_deg,\n",
    "        \"d_tc\": d_tc,\n",
    "        \"ClosestK\": ClosestK,\n",
    "        \"Delta_theta_act_in\": Delta_theta_act_in,\n",
    "        \"Delta_theta_act_out\": Delta_theta_act_out,\n",
    "        \"Delta_dist_act_in\": Delta_dist_act_in,\n",
    "        \"Delta_dist_act_out\": Delta_dist_act_out,\n",
    "        \"d_act_curr_in\": d_act_curr_in,\n",
    "        \"d_act_curr_out\": d_act_curr_out,\n",
    "        \"d_act_target_in\": d_act_target_in,\n",
    "        \"d_act_target_out\": d_act_target_out,\n",
    "        \"img_path_in\": f\"sae_analysis/out/env_imgs/seed{seed}_step{step}_timestep{timestep}_actionInput.png\",\n",
    "        \"img_path_out\": f\"sae_analysis/out/env_imgs/seed{seed}_step{step}_timestep{timestep}_actionOutput.png\"\n",
    "    }\n",
    "\n",
    "def round_to_significant_digits(val):\n",
    "    if isinstance(val, (int, float)):  # Check if the value is numeric\n",
    "        return float(f\"{val:.3g}\")  # Format to 3 significant digits\n",
    "    return val  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process each selected feature index\n",
    "for feature_idx in range(1024, 2048):\n",
    "    print(f\"Processing feature index: {feature_idx}\")\n",
    "    results = []\n",
    "    try:\n",
    "        top_activations = better_acts.sort_values(\n",
    "    f\"feature_{feature_idx}\", ascending=False).head(20)\n",
    "    except Exception as e:\n",
    "        print(\"Skipping feature \", feature_idx)\n",
    "        continue\n",
    "    latent_df = top_activations[['seed', 'step_idx', 'timestep', f'feature_{feature_idx}']]\n",
    "    non_na_count = better_acts[f\"feature_{feature_idx}\"].notna().sum()\n",
    "    if non_na_count < 50:\n",
    "        print(f\"Skipping feature {feature_idx} due to insufficient activations\")\n",
    "        continue\n",
    "    for idx, row in latent_df.iterrows():\n",
    "        stats = compute_stats(row, policy)\n",
    "        results.append(stats)\n",
    "    # Convert the results into a DataFrame\n",
    "    results_df = pd.DataFrame(results).sort_values(\"activation\", ascending=False)\n",
    "    # Reduce precision of numerical values to 3 significant digits\n",
    "    results_df = results_df.apply(\n",
    "        lambda col: col.map(round_to_significant_digits) if col.dtype in [np.float64, np.int64] else col\n",
    "    )\n",
    "    # Save the DataFrame to a CSV file\n",
    "    results_df.to_csv(f\"sae_analysis/out/new/f{feature_idx}_stats.csv\", index=False)\n",
    "    print(f\"Saved CSV for feature index: {feature_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_features_with_non_na = 0\n",
    "\n",
    "for feature_idx in range(2048):\n",
    "    # print(f\"Processing feature index: {feature_idx}\")\n",
    "    try:\n",
    "        # Check the number of non-NA values in the feature column\n",
    "        non_na_count = better_acts[f\"feature_{feature_idx}\"].notna().sum()\n",
    "        \n",
    "        if non_na_count > 10:\n",
    "            print(f\"Feature index: {feature_idx}, Non-NA count: {non_na_count}\")\n",
    "            count_features_with_non_na += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        # print(\"Skipping feature \", feature_idx)\n",
    "        continue\n",
    "\n",
    "print(f\"Number of features with more than 50 non-NA values: {count_features_with_non_na}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22440"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30*748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.233333333333333"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5*748/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a file name exists in a folder\n",
    "import os\n",
    "os.path.exists(\"sae_analysis/out/env_imgs/seed0_step0_timestep0_actionInput.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1933799/1231549832.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  inf_inputs = torch.load(f\"data/inference_inputs_seed_{int(seed)}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 27242, Step: 112, Timestep: 3\n",
      "Theta_tc: 4.292908334280913, d_tc: 5.892085507583355, ClosestK: 5 \n",
      " Delta_theta_act_in: 228.07397729338925 Delta_dist_act_out: 8.524491558907302 \n",
      " Delta_dist_act_in: 4.612924313909092 Delta_dist_act_out: 8.524491558907302 \n",
      " d_act_curr_in: 12.104566669028511 d_act_curr_out: 12.180254695448014 \n",
      " d_act_target_in: 15.469839999955132 d_act_target_out: 15.67071618704117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_hzhang2_umass_edu/jnainani_umass_edu/diffusion_policy_interp/diffusion_policy/env/pusht/pymunk_keypoint_manager.py:37: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap('tab10')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Seed: 30449, Step: 160, Timestep: 1\n",
      "Theta_tc: -2.9033618997715966, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: 2.857662356274125 Delta_dist_act_out: 7.796555249261298 \n",
      " Delta_dist_act_in: 8.82078920555269 Delta_dist_act_out: 7.796555249261298 \n",
      " d_act_curr_in: 13.831234245375066 d_act_curr_out: 13.755010382304548 \n",
      " d_act_target_in: 16.247847524461836 d_act_target_out: 15.868393522531587\n",
      "\n",
      "\n",
      "Seed: 30449, Step: 160, Timestep: 0\n",
      "Theta_tc: -2.9033618997715966, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: -73.89350802642849 Delta_dist_act_out: 6.8138331835571355 \n",
      " Delta_dist_act_in: 7.796555249261298 Delta_dist_act_out: 6.8138331835571355 \n",
      " d_act_curr_in: 13.755010382304548 d_act_curr_out: 13.503240927422599 \n",
      " d_act_target_in: 15.868393522531587 d_act_target_out: 15.520998375859582\n",
      "\n",
      "\n",
      "Seed: 27242, Step: 112, Timestep: 6\n",
      "Theta_tc: 4.292908334280913, d_tc: 5.892085507583355, ClosestK: 5 \n",
      " Delta_theta_act_in: -37.0210391506273 Delta_dist_act_out: 8.009703604550946 \n",
      " Delta_dist_act_in: 3.7091172114748474 Delta_dist_act_out: 8.009703604550946 \n",
      " d_act_curr_in: 10.340604738909029 d_act_curr_out: 11.074008723963415 \n",
      " d_act_target_in: 14.384016292012882 d_act_target_out: 15.056624229773144\n",
      "\n",
      "\n",
      "Seed: 30449, Step: 160, Timestep: 5\n",
      "Theta_tc: -2.9033618997715966, d_tc: 6.343284254644151, ClosestK: 2 \n",
      " Delta_theta_act_in: 20.106273746576655 Delta_dist_act_out: 12.08502802678232 \n",
      " Delta_dist_act_in: 6.484109555375557 Delta_dist_act_out: 12.08502802678232 \n",
      " d_act_curr_in: 14.539152801693138 d_act_curr_out: 13.843240651442585 \n",
      " d_act_target_in: 16.887147508851697 d_act_target_out: 16.43755218360552\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f922_30 = better_acts.sort_values(\n",
    "    f\"feature_{feature_idx}\", ascending=False\n",
    ")[['seed','step_idx','timestep', f'feature_{feature_idx}']].head(5)\n",
    "for idx, row in f922_30.iterrows():\n",
    "    seed, step, timestep, _ = row.values\n",
    "    seed, step, timestep = int(seed), int(step), int(timestep)\n",
    "    inf_inputs = torch.load(f\"data/inference_inputs_seed_{int(seed)}.pt\")\n",
    "    time_ind = 99 - int(timestep)\n",
    "    obs = torch.Tensor(inf_inputs[int(step)][time_ind]['cond_input'])\n",
    "    action_in = torch.Tensor(inf_inputs[int(step)][time_ind]['trajectory_input'])\n",
    "    action_out = torch.Tensor(inf_inputs[int(step)][time_ind]['trajectory_output'])\n",
    "    obs_norm1 = policy.normalizer['obs'].unnormalize(obs)[0][0]\n",
    "    obs_norm2 = policy.normalizer['obs'].unnormalize(obs)[0][1]\n",
    "    inp_action = np.array(policy.normalizer['action'].unnormalize(action_in)[0].detach().cpu()) / 512 * 96\n",
    "    out_action = np.array(policy.normalizer['action'].unnormalize(action_out)[0].detach().cpu()) / 512 * 96\n",
    "    points1 = np.array(obs_norm1.detach().cpu()).reshape(-1, 2)  / 512 * 96\n",
    "    points2 = np.array(obs_norm2.detach().cpu()).reshape(-1, 2)  / 512 * 96\n",
    "    print(f\"Seed: {seed}, Step: {step}, Timestep: {timestep}\")\n",
    "    # print(theta_tc(points1))\n",
    "    _, theta_tc_deg, _, theta_c_deg = theta_tc(points2)\n",
    "    # print(dist_tc(points1))\n",
    "    d_tc = dist_tc(points2)\n",
    "    ClosestK = Ka(points2)\n",
    "    # print(Ka(points2))\n",
    "    _, _, _, Delta_theta_act_in = theta_action(inp_action)\n",
    "    _, _, _, Delta_theta_act_out = theta_action(out_action)\n",
    "    Delta_dist_act_in = dist_action(inp_action)\n",
    "    Delta_dist_act_out = dist_action(out_action)\n",
    "    # print(dist_action_mid(inp_action, points1))\n",
    "    d_act_curr_in = dist_action_mid(inp_action, points2)\n",
    "    d_act_curr_out = dist_action_mid(out_action, points2)\n",
    "    d_act_target_in = dist_action_target(inp_action)\n",
    "    d_act_target_out = dist_action_target(out_action)\n",
    "    print(f\"Theta_tc: {theta_tc_deg}, d_tc: {d_tc}, ClosestK: {ClosestK} \\n Delta_theta_act_in: {Delta_theta_act_in} Delta_dist_act_out: {Delta_dist_act_out} \\n Delta_dist_act_in: {Delta_dist_act_in} Delta_dist_act_out: {Delta_dist_act_out} \\n d_act_curr_in: {d_act_curr_in} d_act_curr_out: {d_act_curr_out} \\n d_act_target_in: {d_act_target_in} d_act_target_out: {d_act_target_out}\")\n",
    "    get_image_from_obs(points2, inp_action, seed, step, timestep, action_mode=\"Input\")\n",
    "    get_image_from_obs(points2, out_action, seed, step, timestep, action_mode=\"Output\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1886101/799738041.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  inf_inputs = torch.load(f\"data/inference_inputs_seed_{int(seed)}.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8350756\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example inputs\n",
    "\n",
    "for idx, row in f922_30.iterrows():\n",
    "    seed, step, timestep, _ = row.values\n",
    "    inf_inputs = torch.load(f\"data/inference_inputs_seed_{int(seed)}.pt\")\n",
    "    time_ind = 99 - int(timestep)\n",
    "    obs = torch.Tensor(inf_inputs[int(step)][time_ind]['cond_input'])\n",
    "    action_in = torch.Tensor(inf_inputs[int(step)][time_ind]['trajectory_input'])\n",
    "    action_out = torch.Tensor(inf_inputs[int(step)][time_ind]['trajectory_output'])\n",
    "    break\n",
    "obs = policy.normalizer['obs'].unnormalize(obs)[0][0]  # torch.Size([1, 2, 20])\n",
    "\n",
    "# Extract the points in (x, y) format\n",
    "# Reshape the last dimension into 10 pairs of (x, y)\n",
    "points = np.array(obs.detach().cpu()).reshape(-1, 2)\n",
    "\n",
    "# Rescale points from [0, 512] to [0, 96]\n",
    "points = points / 512 * 96\n",
    "target_action_norm = np.array(policy.normalizer['action'].unnormalize(action_in)[0].detach().cpu()) / 512 * 96\n",
    "# Center of the image\n",
    "center_x, center_y = 96 / 2, 96 / 2\n",
    "\n",
    "# Distance and angle\n",
    "d_i = 30\n",
    "angle = (5 * np.pi) / 4  # 45 degrees\n",
    "\n",
    "# Endpoint of the line\n",
    "end_x = center_x + d_i * np.cos(angle)\n",
    "end_y = center_y - d_i * np.sin(angle)  # Subtract because y-coordinates increase downwards in images\n",
    "\n",
    "_, theta_tc_deg, theta_c_rad, theta_c_deg = theta_tc(points)\n",
    "print(theta_c_rad)\n",
    "\n",
    "points_to_average = [\n",
    "    points[8], points[1] #, points[2]\n",
    "]\n",
    "\n",
    "# Convert to a NumPy array for easy computation\n",
    "points_array = np.array(points_to_average)\n",
    "\n",
    "# Compute the average\n",
    "average_point = np.mean(points_array, axis=0)\n",
    "\n",
    "end_x_curr = average_point[0] + d_i * np.cos(theta_c_rad + np.pi)\n",
    "end_y_curr = average_point[1] - d_i * np.sin(theta_c_rad + np.pi)  # Subtract because y-coordinates increase downwards in images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = PushTKeypointsEnv()\n",
    "# env.reset()\n",
    "canvas = pygame.Surface((env.window_size, env.window_size))\n",
    "canvas.fill((255, 255, 255))\n",
    "env.screen = canvas\n",
    "draw_options = DrawOptions(canvas)\n",
    "\n",
    "# Draw goal pose.\n",
    "env.space = pymunk.Space()\n",
    "goal_body = env._get_goal_pose_body(np.array([256,256,np.pi/4]))\n",
    "env.goal_color = pygame.Color('LightGreen')\n",
    "block_temp = env.add_tee((256, 300), 0) #(average_point[0] / 96 * 512, average_point[1] / 96 * 512), theta_c_rad ) #+ np.pi)\n",
    "for shape in block_temp.shapes:\n",
    "    goal_points = [pymunk.pygame_util.to_pygame(goal_body.local_to_world(v), draw_options.surface) for v in shape.get_vertices()]\n",
    "    goal_points += [goal_points[0]]\n",
    "    pygame.draw.polygon(canvas, env.goal_color, goal_points)\n",
    "\n",
    "# env.space.debug_draw (draw_options)\n",
    "img = np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "img = cv2.resize(img, (env.render_size, env.render_size))\n",
    "img = Image.fromarray(img)\n",
    "# display(img)\n",
    "\n",
    "# Plot the image\n",
    "image = img  # (96, 96, 3)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "\n",
    "# Overlay the points and annotate them\n",
    "for idx, (x, y) in enumerate(points):\n",
    "    plt.scatter(x, y, c='red', label=f'Point {idx}' if idx == 0 else None)  # Points now match image size\n",
    "    plt.text(x, y, f'{idx}', fontsize=9, color='blue')\n",
    "\n",
    "# for idx, (x, y) in enumerate(target_action_norm):\n",
    "#     plt.scatter(x, y, c='yellow', label=f'Point {idx}' if idx == 0 else None)  # Points now match image size\n",
    "#     plt.text(x, y, f'{idx}', fontsize=9, color='red')\n",
    "\n",
    "# Plot arrows for the action trajectory\n",
    "# for i in range(len(target_action_norm) - 1):\n",
    "#     start_x_act, start_y_act = target_action_norm[i]\n",
    "#     end_x_act, end_y_act = target_action_norm[i + 1]\n",
    "#     dx = end_x_act - start_x_act\n",
    "#     dy = end_y_act - start_y_act\n",
    "#     plt.quiver(start_x_act, start_y_act, dx, dy, angles='xy', scale_units='xy', scale=1, color='purple')\n",
    "\n",
    "top_left = points[1]     # Left side of the base\n",
    "top_right = points[2]    # Right side of the base\n",
    "center_top = points[4]   # Top of the T (center vertical bar)\n",
    "mid_point = average_point    # Middle connecting point\n",
    "\n",
    "# Draw the base of the T (horizontal line)\n",
    "plt.plot([top_left[0], top_right[0]], [top_left[1], top_right[1]], c='grey', linewidth=2)\n",
    "\n",
    "# Draw the vertical bar of the T\n",
    "plt.plot([mid_point[0], center_top[0]], [mid_point[1], center_top[1]], c='grey', linewidth=2)\n",
    "\n",
    "# Plot the center of the T\n",
    "# plt.scatter(mid_point[0], mid_point[1], c='purple', label='Center', s=100)\n",
    "\n",
    "\n",
    "# Plot the center of the image\n",
    "plt.scatter(center_x, center_y, c='green', label='Center')  # Center of the image\n",
    "# plt.scatter(256/512*96, 300/512*96, c='grey', label='Center')  # Center of the image\n",
    "\n",
    "# Plot the average point\n",
    "# plt.scatter(average_point[0], average_point[1], c='orange', label='Average Point')\n",
    "# plt.scatter(1, 1, c='yellow', label='Action')  # Center of the image\n",
    "\n",
    "# Draw the line\n",
    "# plt.plot([center_x, end_x], [center_y, end_y], c='purple', label=f'Line d_i={d_i}, angle=pi/4')\n",
    "# plt.plot([average_point[0], end_x_curr], [average_point[1], end_y_curr], c='blue', label=f'Line d_i={d_i}, angle=pi/4')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Points and Line Overlay on Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9269908169872414"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(5 * np.pi) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261.30478922526044, 268.466796875)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_point[0] / 96 * 512, average_point[1] / 96 * 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([51.139698, 52.359108], dtype=float32),\n",
       " array([39.729767, 41.35932 ], dtype=float32),\n",
       " array([56.114487, 57.294136], dtype=float32))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[8], points[1], points[2]\n",
    "# avg of the above points \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Point: [48.994648 50.337524]\n"
     ]
    }
   ],
   "source": [
    "points_to_average = [\n",
    "    points[8], points[1], points[2]\n",
    "]\n",
    "\n",
    "# Convert to a NumPy array for easy computation\n",
    "points_array = np.array(points_to_average)\n",
    "\n",
    "# Compute the average\n",
    "average_point = np.mean(points_array, axis=0)\n",
    "\n",
    "print(\"Average Point:\", average_point)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
